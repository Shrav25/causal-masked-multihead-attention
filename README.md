# Causal Masked Multi-Head Attention from Scratch

This repository demonstrates how to build a **Causal Masked Multi-Head Attention** mechanism using PyTorch. It's a key component of models like GPT.

---

## ğŸ” What is Causal Masked Attention?

Causal masking ensures that position `i` can only attend to positions `â‰¤ i`, which is essential for autoregressive generation.
Multi-head attention allows the model to jointly attend to information from different representation subspaces.

---

## ğŸ“¦ Folder Structure

- `attention/` â€” Core implementation:
  - `multihead_attention.py` â€” Multi-head attention with causal masking.
  - `causal_mask.py` â€” Causal mask generator.
- `examples/` â€” Run toy examples and visualizations.
- `tests/` â€” Unit tests.

---

## ğŸ§  Code Highlights

```python
# Causal mask shape: [batch, 1, seq_len, seq_len]
mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(1)
